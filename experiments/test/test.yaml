run_or_experiment: "SupervisedLearningTrainable"

resources_per_trial:
    cpu: 1
    gpu: "fit_cpu"
    # debug
    # gpu: 0
stop:
    is_num_iterations_reached: 1
checkpoint_freq: 0
checkpoint_at_end: False

config:
    version: 1.1

    num_iterations: 64

    device: "torch.device('cpu')"
    # debug
    # device: "torch.device('cpu')"

    seed:
        grid_search:
            - 1482555873
            # - 698841058
            # - 2283198659
            # - 1855152618
            # - 2444264003
            # - 4191194675
            # - 2563230118
            # - 2508066235
            # - 690101352
            # - 1489128536
            # - 193493729
            # - 3247095100
            # - 2569134589
            # - 3859394752
            # - 3982413761
            # - 2889788203
            # - 3507183834
            # - 1288605031
            # - 1603403502
            # - 4110491292
            # - 3228301166
            # - 1625209643
            # - 3229422879
            # - 4117841395
            # - 383017664
            # - 369370935
            # - 2278516753
            # - 2326123831
            # - 3558864999
            # - 281470168
            # - 3965936537
            # - 1095454788
            # - 4151697083
            # - 3549593167
            # - 3621705125
            # - 2951949230
            # - 2942809220
            # - 1412354999
            # - 3653984540
            # - 38155820
            # - 452984486
            # - 2219886835
            # - 1824037622
            # - 1223472929
            # - 375839252
            # - 2597045926
            # - 187775831
            # - 2291831353
            # - 3551958879
            # - 760971382
            # - 1019978323
            # - 3385238229
            # - 2124033150
            # - 2909826692
            # - 3761144171
            # - 2586511809
            # - 2821469938
            # - 3244598120
            # - 1195937429
            # - 3800305993
            # - 1106674707
            # - 1922347502
            # - 2999545244
            # - 1650175939
            # - 3200709490
            # - 1947803234
            # - 301456582
            # - 1611073380
            # - 3238577641
            # - 1446155378
            # - 1705511488
            # - 2777770570
            # - 3913116301
            # - 1525032703
            # - 3260116528
            # - 3235491768
            # - 2021899074
            # - 550305527
            # - 2227549273
            # - 3227763636
            # - 4034863635
            # - 2984716302
            # - 822586165
            # - 2244632731
            # - 2578193189
            # - 2806006426
            # - 364049366
            # - 2580805533
            # - 1471857781
            # - 636897984
            # - 3061662337
            # - 3640170982
            # - 3927284778
            # - 3117797531
            # - 1117650596
            # - 223429686
            # - 651134664
            # - 955904314
            # - 1703657804
            # - 2162018890

    dataset: FashionMNIST

    batch_size: 1

    num_batch_per_iteration: 200

    # exec-code before/after the setup of the specified Trainable
    before_DatasetLearningTrainable_setup_code: |-
        def data_loader_fn(dataset, train, batch_size):
            transform_list = []
            if dataset=="CIFAR10":
                transform_list.append(transforms.Grayscale())
            transform_list.append(transforms.ToTensor())
            transform_list.append(utils.transforms_flatten)
            return DataLoader(
                dataset_utils.partial_dateset(
                    eval(f"datasets.{dataset}")(
                        os.environ.get('DATA_DIR'),
                        train=train,
                        download=False,
                        transform=transforms.Compose(
                            transform_list
                        ),
                        target_transform=transforms.Compose(
                            [
                                transforms.Lambda(
                                    lambda idx: utils.np_idx2onehot(idx, 10)
                                ),
                            ]
                        )
                    ),
                    partial_num=6000,
                ),
                batch_size=batch_size,
                num_workers=1,
                pin_memory=True,
                shuffle=True,
            )
    data_packs:
        train:
            data_loader: |-
                data_loader_fn(self.config['dataset'], True, self.config['batch_size'])
            do: "['learn']"
        # test:
        #     data_loader: |-
        #         data_loader_fn(self.config['dataset'], False, 60)
        #     do: "['predict']"

    predictive_coding:
        grid_search:
            - True
            # - False

    T: 64

    PCTrainer_kwargs:
        T: "self.config['T'] if self.config['predictive_coding'] else 1"

        update_x_at: "all"
        optimizer_x_fn: "optim.SGD"
        optimizer_x_kwargs:
            lr: 0.1
        x_lr_discount: 0.5
        x_lr_amplifier: 1.0

        update_p_at: "last"
        optimizer_p_fn: "optim.SGD"
        optimizer_p_kwargs:
            lr:
                grid_search:
                    # for FashionMNIST
                    - 0.05
                    - 0.10
                    - 0.15
                    - 0.20
                    - 0.25
                    - 0.30
                    # # for CIFAR10
                    # - 0.25
                    # - 0.1
                    # - 0.075
                    # - 0.05
                    # - 0.025
                    # - 0.01
                    # - 0.005

        plot_progress_at: "[]"

    model_creation_code: |-

        # import
        import predictive_coding as pc
        import torch.optim as optim

        if self.config['dataset']=="CIFAR10":
            input_size = 1024
        elif self.config['dataset']=="FashionMNIST" or self.config['dataset']=="MNIST":
            input_size = 784
        else:
            raise NotImplementedError

        # create model
        self.model = [
            nn.Linear(input_size, 32, bias=False),
            pc.PCLayer(),
            nn.Sigmoid(),
            nn.Linear(32, 32, bias=False),
            pc.PCLayer(),
            nn.Sigmoid(),
            nn.Linear(32, 10, bias=False),
        ]

        # decide pc_layer
        for model_ in self.model:
            if isinstance(model_, pc.PCLayer):
                if not self.config['predictive_coding']:
                    self.model.remove(model_)

        # init
        for model_ in self.model:
            if isinstance(model_, nn.Linear):
                torch.nn.init.xavier_uniform_(model_.weight, gain=1)

        # create sequential
        self.model = nn.Sequential(*self.model).to(self.device)

        # create pc_trainer
        self.config['PCTrainer_kwargs']['T']=eval(
            self.config['PCTrainer_kwargs']['T']
        )
        self.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(
            self.config['PCTrainer_kwargs']['optimizer_x_fn']
        )
        self.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(
            self.config['PCTrainer_kwargs']['optimizer_p_fn']
        )
        self.config['PCTrainer_kwargs']['plot_progress_at']=eval(
            self.config['PCTrainer_kwargs']['plot_progress_at']
        )
        self.pc_trainer = pc.PCTrainer(
            self.model,
            **self.config['PCTrainer_kwargs'],
        )

    # predict_code: |-
    #     self.model.eval()
    #     prediction = self.model(data)
    #     self.classification_error = utils.get_classification_error(
    #         prediction, target
    #     )

    # train_on_batch_kwargs:
    #     is_log_progress: False
    #     is_return_results_every_t: False
    #     is_checking_after_callback_after_t: False

    learn_code: |-

        if batch_idx<self.config['num_batch_per_iteration']:

            self.model.train()

            def loss_fn(outputs, target):
                print(outputs)
                return (outputs - target).pow(2).sum() * 0.5

            self.pc_trainer.train_on_batch(
                data, loss_fn,
                loss_fn_kwargs={'target':target},
                **self.config['train_on_batch_kwargs'],
            )

    log_packs:
        classification_error:
            log: "self.classification_error.item()"
            at_data_pack: "['test']"

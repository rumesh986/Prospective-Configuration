run_or_experiment: "SupervisedLearningTrainable"

resources_per_trial:
    cpu: 8
    # gpu: "fit_cpu"
    # gpu: 1
    gpu: 0
stop:
    is_num_iterations_reached: 1
checkpoint_freq: 0
checkpoint_at_end: False

config:
    version: 1.0

    num_iterations: 1

    # device: "torch.device('cuda')"
    device: "torch.device('cpu')"

    seed: 698841058

    dataset: FashionMNIST
    targets: "[1,8]"

    batch_size: 1

    num_batch_per_iteration: 10
    num_test: 10

    num_test_per_target: 10
    num_train_per_target: 10

    before_DatasetLearningTrainable_setup_code: |-
        def data_loader_fn(dataset, train, batch_size, targets, num):
            print(f"Loading data training: {train}")
            targets = eval(targets)
            transform_list = []
            transform_list.append(transforms.ToTensor())
            transform_list.append(utils.transforms_flatten)
            return DataLoader(
                dataset_utils.partial_dateset(
                    eval(f"datasets.{dataset}")(
                        os.environ.get('DATA_DIR'),
                        train=train,
                        download=False,
                        transform=transforms.Compose(transform_list),
                        target_transform=transforms.Compose([transforms.Lambda(lambda idx: utils.np_idx2onehot(targets.index(idx), len(targets)))]),
                    ),
                    partial_targets=targets,
                    partial_num=num,
                ),
                batch_size=batch_size,
                num_workers=1,
                pin_memory=True,
                shuffle=True
            )
    data_packs:
        train:
            data_loader: |-
                data_loader_fn(self.config['dataset'], True, self.config['batch_size'], self.config['targets'], self.config['num_train_per_target'])
            do: "['learn']"
        test:
            data_loader: |-
                data_loader_fn(self.config['dataset'], False, 1, self.config['targets'], self.config['num_test_per_target'])
            do: "['predict']"
    
    predictive_coding: True

    PCTrainer_kwargs:
        # T: "self.config['T'] if self.config['predictive_coding'] else 1"
        T: 128

        update_x_at: "all"
        # optimizer_x_fn: "optim.SGD"
        optimizer_x_kwargs:
            lr: 0.1
        x_lr_discount: 0.5
        x_lr_amplifier: 1.0

        update_p_at: "last"
        # optimizer_p_fn: eval("optim.SGD")
        optimizer_p_kwargs:
            lr: 0.1
        
        
        # early_stop_condition: self._check_early_stop()
        early_stop_condition: True if (self._optimizer_x.param_groups[0]['lr'] <= (self._x_lr_discount ** 2) * self._optimizer_x_kwargs["lr"]) else False
        
        # plot_progress_at: "[]"
        # callback_after_backward: |-
        #     def callback_after_backward(t, **kwargs):
        #         print(f"##### t: {t} ######")
    
    model_creation_code: |-
        import predictive_coding as pc
        import torch.optim as optim

        self.targets = eval(self.config['targets'])

        input_size = 784
        output_size = len(self.targets)

        self.model = [
            nn.Linear(input_size, 32, bias=False),
            pc.PCLayer(),
            nn.Sigmoid(),
            nn.Linear(32, 32, bias=False),
            pc.PCLayer(),
            nn.Sigmoid(),
            nn.Linear(32, output_size, bias=False)
        ]

        for model_ in self.model:
            if isinstance(model_, nn.Linear):
                torch.nn.init.xavier_uniform(model_.weight, gain=1)
        
        self.model = nn.Sequential(*self.model).to(self.device)

        self.config['PCTrainer_kwargs']['optimizer_x_fn']=eval("optim.SGD")
        self.config['PCTrainer_kwargs']['optimizer_p_fn']=eval("optim.SGD")
        # self.config['PCTrainer_kwargs']['plot_progress_at']=eval("[]")
        self.config['PCTrainer_kwargs']['plot_progress_at']=eval("'all'")

        # self.config['PCTrainer_kwargs']['callback_after_backward']=callback


        # print(f"PLOT PROGRESS AT: {self.config['PCTrainer_kwargs']['plot_progress_at']} {type(self.config['PCTrainer_kwargs']['plot_progress_at'])} ########")

        self.pc_trainer = pc.PCTrainer(self.model, **self.config['PCTrainer_kwargs'])
    
    predict_code: |-
        print(f"{batch_idx}/{self.config['num_test']}")
        if batch_idx < self.config['num_test']:
            self.model.eval()   #set model to eval mode (opposite of self.model.train(), doesnt actually compute anything)
            prediction = self.model(data)
            self.classification_error = utils.get_classification_error(prediction, target)

            total_runs = prediction.size()[0]
            correct_predictions = 0
            for i in range(total_runs):
                pred_val = self.targets[torch.argmax(prediction[i])]
                target_val = self.targets[torch.argmax(target[i])]
                print(f"Target: {target_val}, Prediciton: {pred_val}")
                if target_val == pred_val:
                    correct_predictions += 1

                self.pc_trainer.save_prediction_results(batch_idx, target_val, pred_val)            

            print(f"[{batch_idx}] {correct_predictions}/{total_runs} | accuracy = {correct_predictions/total_runs} | err = {self.classification_error}")

    
    train_on_batch_kwargs:
        is_log_progress: True
        is_return_results_every_t: True
        is_checking_after_callback_after_t: False

    learn_code: |-
        if batch_idx < self.config['num_batch_per_iteration']:
            # print(f"## LEARNING {batch_idx}")
            # print(data.size())
            # print(target.size())

            self.model.train()  # set model in train mode (doesnt actually compute any training)

            # just their energy function
            def loss_fn(outputs, target):
                return (outputs - target).pow(2).sum() * 0.5
            

            def callback(t, selftest):
                print(f"##### t: {t} ######")
                # print(self.model.)
                # for x in self.get_model_xs():
                #     print(x)
                model_xs = list(selftest.get_model_xs())
                print(model_xs[0].grad)

            self.pc_trainer.train_on_batch(
                data, 
                loss_fn, 
                loss_fn_kwargs={'target': target},
                # callback_after_backward=callback,
                # callback_after_backward_kwargs={'selftest': self},
                **self.config['train_on_batch_kwargs'],
            )

    log_packs:
        classification_error:
            log: "self.classification_error.item()"
            at_data_pack: "['test']"